{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dcc81e3-213b-48c2-a4a6-95c4a6435989",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac212903-dc2a-4e36-9d10-24e09cd4cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import welch\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report,\n",
    "                             roc_curve, auc)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7823a-a4d4-40f1-9ac2-016fde040ba3",
   "metadata": {},
   "source": [
    "## Defining pathway for the base dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97195c6c-f710-4e36-8ad9-4e0f91dae8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders ready:\n",
      " - csv: C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\corrected_csv\n",
      " - features: C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\features\n",
      " - plots: C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\plots\n",
      " - similarity: C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\similarity\n",
      " - models: C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\models\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR  = r\"C:\\Users\\Admin\\Desktop\\ALVIN\\dataset\"   # folder with CSV/EDF\n",
    "OUTPUT_DIR = r\"C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\"   # where results go\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Organized subfolders\n",
    "DIRS = {\n",
    "    \"csv\":        os.path.join(OUTPUT_DIR, \"corrected_csv\"),\n",
    "    \"features\":   os.path.join(OUTPUT_DIR, \"features\"),\n",
    "    \"plots\":      os.path.join(OUTPUT_DIR, \"plots\"),\n",
    "    \"similarity\": os.path.join(OUTPUT_DIR, \"similarity\"),\n",
    "    \"models\":     os.path.join(OUTPUT_DIR, \"models\"),\n",
    "}\n",
    "for d in DIRS.values():\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# ---------------- Analysis parameters ----------------\n",
    "WINDOW_SEC   = 5.0       # length of each window in seconds\n",
    "OVERLAP      = 0.5       # 50% overlap\n",
    "DURATION_SEC = 60.0      # use first 60 s (for EDF) / infer fs from rows/60 (for CSV)\n",
    "FS_KNOWN     = None      # set e.g. 516.67 if you know exact fs; else leave None\n",
    "RANDOM_SEED  = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# 19-channel target order\n",
    "CHANNELS = ['Fp1','Fp2','F3','F4','F7','F8','T3','T4','C3','C4',\n",
    "            'T5','T6','P3','P4','O1','O2','Fz','Cz','Pz']\n",
    "\n",
    "# EEG bands\n",
    "BANDS = {\n",
    "    \"Delta\": (0.5, 4.0),\n",
    "    \"Theta\": (4.0, 7.0),\n",
    "    \"Alpha\": (8.0, 13.0),\n",
    "    \"Beta\":  (13.0, 30.0),\n",
    "    \"Gamma\": (30.0, 45.0),  # stop at 45 Hz to avoid 50 Hz region\n",
    "}\n",
    "\n",
    "print(\"Folders ready:\")\n",
    "for k, v in DIRS.items():\n",
    "    print(f\" - {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd410e-25d7-423d-9a16-6896cae3a952",
   "metadata": {},
   "source": [
    "## Loader, Header fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed3a7cc4-d6ef-4c85-be57-83500d67508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likely_unlabeled(df_head, expected):\n",
    "    cols = list(df_head.columns)\n",
    "    if cols == expected:\n",
    "        return False\n",
    "    num_like = 0\n",
    "    for c in cols:\n",
    "        try:\n",
    "            float(str(c).strip()); num_like += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "    return num_like >= len(cols) // 2\n",
    "\n",
    "def ensure_labeled_csv(path, expected_channels=CHANNELS, corrected_dir=DIRS[\"csv\"]):\n",
    "    \"\"\"Load CSV; if headers missing/misaligned, fix to the 19-channel order.\"\"\"\n",
    "    stem = os.path.splitext(os.path.basename(path))[0]\n",
    "    try:\n",
    "        df_try = pd.read_csv(path, nrows=3)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        df.columns = expected_channels\n",
    "        df.to_csv(os.path.join(corrected_dir, f\"{stem}_corrected.csv\"), index=False)\n",
    "        return df, stem\n",
    "\n",
    "    if likely_unlabeled(df_try, expected_channels):\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        df.columns = expected_channels\n",
    "        df.to_csv(os.path.join(corrected_dir, f\"{stem}_corrected.csv\"), index=False)\n",
    "        return df, stem\n",
    "    else:\n",
    "        df = pd.read_csv(path)\n",
    "        if list(df.columns) != expected_channels:\n",
    "            if set(expected_channels).issubset(df.columns):\n",
    "                df = df[expected_channels]\n",
    "            else:\n",
    "                raise ValueError(f\"{path}: columns do not match expected 19 channels.\")\n",
    "        return df, stem\n",
    "\n",
    "def load_edf_as_df(path, target_channels=CHANNELS, duration_sec=DURATION_SEC):\n",
    "    \"\"\"Read EDF, remap modern names to your legacy targets, return first 60 s as DataFrame.\"\"\"\n",
    "    import mne\n",
    "    ALIAS = {'T7':'T3','T8':'T4','P7':'T5','P8':'T6'}  # modern -> legacy\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=\"ERROR\")\n",
    "    raw.rename_channels({ch: ALIAS.get(ch, ch) for ch in raw.ch_names})\n",
    "    fs = float(raw.info['sfreq'])\n",
    "    n = int(duration_sec * fs)\n",
    "    picks = [ch for ch in target_channels if ch in raw.ch_names]\n",
    "    if not picks:\n",
    "        raise ValueError(f\"{path}: none of the 19 target channels present.\")\n",
    "    data = raw.get_data(picks=picks)[:, :n].T\n",
    "    # pre-allocate with correct number of rows (avoid length mismatch)\n",
    "    df = pd.DataFrame(index=np.arange(data.shape[0]), columns=target_channels, dtype=float)\n",
    "    df[:] = np.nan\n",
    "    have = pd.DataFrame(data, columns=picks)\n",
    "    for ch in picks:\n",
    "        df[ch] = have[ch].values\n",
    "    return df, fs\n",
    "\n",
    "def load_eeg_as_df(path, target_channels=CHANNELS, duration_sec=DURATION_SEC):\n",
    "    \"\"\"Wrapper that accepts .csv or .edf and returns (df, subject_id, fs).\"\"\"\n",
    "    stem = os.path.splitext(os.path.basename(path))[0]\n",
    "    ext  = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".csv\":\n",
    "        df, subj = ensure_labeled_csv(path, expected_channels=target_channels, corrected_dir=DIRS[\"csv\"])\n",
    "        fs = df.shape[0] / float(duration_sec)\n",
    "        return df, subj, fs\n",
    "    elif ext == \".edf\":\n",
    "        df, fs = load_edf_as_df(path, target_channels=target_channels, duration_sec=duration_sec)\n",
    "        return df, stem, fs\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e1804-a41a-41b6-a8eb-d2f995f29c67",
   "metadata": {},
   "source": [
    "## DSP, Welch, bands, windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df459c46-f6b5-4f2b-b05f-8556ce20c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def welch_psd(x, fs, nperseg, noverlap):\n",
    "    return welch(x, fs=fs, nperseg=int(nperseg), noverlap=int(noverlap), detrend='constant')\n",
    "\n",
    "def integrate_bandpower(f, Pxx, band):\n",
    "    fmin, fmax = band\n",
    "    idx = (f >= fmin) & (f < fmax)\n",
    "    if not np.any(idx):\n",
    "        return 0.0\n",
    "    return float(np.trapz(Pxx[idx], f[idx]))\n",
    "\n",
    "def compute_wide_summary(df, fs, bands=BANDS, channels=CHANNELS, out_csv=None):\n",
    "    \"\"\"Absolute & relative band powers over full 60 s (one row).\"\"\"\n",
    "    nperseg = int(4 * fs); noverlap = int(0.5 * nperseg)\n",
    "    feats = {}\n",
    "    for ch in channels:\n",
    "        x = pd.to_numeric(df[ch], errors='coerce').fillna(0.0).values\n",
    "        f, Pxx = welch_psd(x, fs, nperseg, noverlap)\n",
    "        total = float(np.trapz(Pxx, f))\n",
    "        for b, rng in bands.items():\n",
    "            abs_p = integrate_bandpower(f, Pxx, rng)\n",
    "            rel_p = abs_p / total if total > 0 else np.nan\n",
    "            feats[f\"{ch}_{b}\"] = abs_p\n",
    "            feats[f\"{ch}_{b}_Rel\"] = rel_p\n",
    "    wide = pd.DataFrame([feats])\n",
    "    if out_csv:\n",
    "        wide.to_csv(out_csv, index=False)\n",
    "    return wide\n",
    "\n",
    "def segment_windows(n_samples, fs, win_sec=WINDOW_SEC, overlap=OVERLAP):\n",
    "    win_len = int(win_sec * fs)\n",
    "    hop = max(1, int(win_len * (1 - overlap)))\n",
    "    return [(s, s + win_len) for s in range(0, n_samples - win_len + 1, hop)]\n",
    "\n",
    "def compute_window_features(df, fs, window, bands=BANDS, channels=CHANNELS):\n",
    "    s, e = window\n",
    "    xw = df.iloc[s:e]\n",
    "    nperseg = max(4, int(2 * fs)); noverlap = int(0.5 * nperseg)\n",
    "    feats = {}\n",
    "    for ch in channels:\n",
    "        x = pd.to_numeric(xw[ch], errors='coerce').fillna(0.0).values\n",
    "        f, Pxx = welch_psd(x, fs, nperseg, noverlap)\n",
    "        total = float(np.trapz(Pxx, f))\n",
    "        for b, rng in bands.items():\n",
    "            abs_p = integrate_bandpower(f, Pxx, rng)\n",
    "            rel_p = abs_p / total if total > 0 else np.nan\n",
    "            feats[f\"{ch}_{b}\"] = abs_p\n",
    "            feats[f\"{ch}_{b}_Rel\"] = rel_p\n",
    "    feats[\"StartSample\"] = s\n",
    "    feats[\"Fs\"] = fs\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70ccd7-b430-4773-b508-28af1f7b18e3",
   "metadata": {},
   "source": [
    "## Process All Subject, Wide summaires Master Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92552153-c513-466e-bb28-b1e05bbb23d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 file(s).\n",
      "[s00] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s01] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s02] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s03] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s04] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s05] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s06] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s07] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s08] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s09] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s10] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s11] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s12] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s13] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s14] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s15] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s16] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s17] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s18] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s19] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s20] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s21] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s22] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s23] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s24] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s25] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s26] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s27] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s28] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s29] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s30] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s31] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s32] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s33] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s34] type=csv, samples=31000, fs=516.667 Hz\n",
      "[s35] type=csv, samples=31000, fs=516.667 Hz\n",
      "\n",
      "Saved master features: C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\features\\eeg_biometric_features_5s_master.csv\n",
      "Subjects processed: ['s00', 's01', 's02', 's03', 's04', 's05', 's06', 's07', 's08', 's09', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21', 's22', 's23', 's24', 's25', 's26', 's27', 's28', 's29', 's30', 's31', 's32', 's33', 's34', 's35']\n"
     ]
    }
   ],
   "source": [
    "# gather CSV + EDF\n",
    "candidates = sorted([p for p in glob.glob(os.path.join(INPUT_DIR, \"*\"))\n",
    "                     if os.path.splitext(p)[1].lower() in [\".csv\", \".edf\"]])\n",
    "if not candidates:\n",
    "    raise FileNotFoundError(f\"No CSV/EDF files found in {INPUT_DIR}\")\n",
    "\n",
    "print(f\"Found {len(candidates)} file(s).\")\n",
    "\n",
    "master_rows = []\n",
    "subjects = []\n",
    "wide_paths = []\n",
    "\n",
    "for path in candidates:\n",
    "    df, subj, fs_file = load_eeg_as_df(path, target_channels=CHANNELS, duration_sec=DURATION_SEC)\n",
    "    fs = FS_KNOWN if FS_KNOWN else fs_file\n",
    "    print(f\"[{subj}] type={os.path.splitext(path)[1].lower()[1:]}, samples={len(df)}, fs={fs:.3f} Hz\")\n",
    "\n",
    "    # Per-subject wide summary (full 60 s)\n",
    "    wide_csv = os.path.join(DIRS[\"features\"], f\"{subj}_bandpowers_wide.csv\")\n",
    "    wide = compute_wide_summary(df, fs, BANDS, CHANNELS, out_csv=wide_csv)\n",
    "    wide_paths.append(wide_csv)\n",
    "    subjects.append(subj)\n",
    "\n",
    "    # Windowed features\n",
    "    windows = segment_windows(len(df), fs, WINDOW_SEC, OVERLAP)\n",
    "    for w in windows:\n",
    "        feats = compute_window_features(df, fs, w, BANDS, CHANNELS)\n",
    "        feats[\"Subject\"] = subj\n",
    "        master_rows.append(feats)\n",
    "\n",
    "# Save master features\n",
    "if not master_rows:\n",
    "    raise RuntimeError(\"No windowed features produced. Check WINDOW_SEC/OVERLAP vs file length.\")\n",
    "\n",
    "master = pd.DataFrame(master_rows)\n",
    "master_csv = os.path.join(DIRS[\"features\"], f\"eeg_biometric_features_{int(WINDOW_SEC)}s_master.csv\")\n",
    "master.to_csv(master_csv, index=False)\n",
    "\n",
    "print(\"\\nSaved master features:\", master_csv)\n",
    "print(\"Subjects processed:\", sorted(set(subjects)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b6297-aad9-46e7-a9d9-bc71504a9605",
   "metadata": {},
   "source": [
    "## Train & Evaluate (RF + SVM, CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5247d4b7-9bc7-4ba5-b7ec-cf59120bd775",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['s00', 's01', 's02', 's03', 's04', 's05', 's06', 's07', 's08', 's09', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21', 's22', 's23', 's24', 's25', 's26', 's27', 's28', 's29', 's30', 's31', 's32', 's33', 's34', 's35']\n",
      "[RandomForest] mean CV acc = 0.9879 | best params: {'clf__max_depth': 18, 'clf__min_samples_leaf': 1, 'clf__n_estimators': 500}\n",
      "[SVM_RBF] mean CV acc = 0.9879 | best params: {'clf__C': 10, 'clf__gamma': 0.001}\n",
      "\n",
      "Saved model summary: C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\models\\model_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mean_cv_acc</th>\n",
       "      <th>confusion_matrix_png</th>\n",
       "      <th>roc_png</th>\n",
       "      <th>verification_csv</th>\n",
       "      <th>class_report_csv</th>\n",
       "      <th>saved_model</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.98793</td>\n",
       "      <td>C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\plots\\cm_...</td>\n",
       "      <td>C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\plots\\roc...</td>\n",
       "      <td>C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\models\\ve...</td>\n",
       "      <td>C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\models\\cl...</td>\n",
       "      <td>C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\models\\be...</td>\n",
       "      <td>{'clf__max_depth': 18, 'clf__min_samples_leaf'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM_RBF</td>\n",
       "      <td>0.98793</td>\n",
       "      <td>C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\plots\\cm_...</td>\n",
       "      <td>C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\plots\\roc...</td>\n",
       "      <td>C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\models\\ve...</td>\n",
       "      <td>C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\models\\cl...</td>\n",
       "      <td>C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\models\\be...</td>\n",
       "      <td>{'clf__C': 10, 'clf__gamma': 0.001}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model  mean_cv_acc  \\\n",
       "0  RandomForest      0.98793   \n",
       "1       SVM_RBF      0.98793   \n",
       "\n",
       "                                confusion_matrix_png  \\\n",
       "0  C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\plots\\cm_...   \n",
       "1  C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\plots\\cm_...   \n",
       "\n",
       "                                             roc_png  \\\n",
       "0  C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\plots\\roc...   \n",
       "1  C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\plots\\roc...   \n",
       "\n",
       "                                    verification_csv  \\\n",
       "0  C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\models\\ve...   \n",
       "1  C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\models\\ve...   \n",
       "\n",
       "                                    class_report_csv  \\\n",
       "0  C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\models\\cl...   \n",
       "1  C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\models\\cl...   \n",
       "\n",
       "                                         saved_model  \\\n",
       "0  C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\models\\be...   \n",
       "1  C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\models\\be...   \n",
       "\n",
       "                                         best_params  \n",
       "0  {'clf__max_depth': 18, 'clf__min_samples_leaf'...  \n",
       "1                {'clf__C': 10, 'clf__gamma': 0.001}  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfm = pd.read_csv(master_csv)\n",
    "X = dfm.drop(columns=[\"Subject\", \"StartSample\", \"Fs\"]).values\n",
    "y = dfm[\"Subject\"].values\n",
    "classes = sorted(dfm[\"Subject\"].unique().tolist())\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"clf\", RandomForestClassifier(random_state=RANDOM_SEED))\n",
    "])\n",
    "rf_grid = {\"clf__n_estimators\":[300, 500], \"clf__max_depth\":[None, 18], \"clf__min_samples_leaf\":[1,2]}\n",
    "\n",
    "svm_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"clf\", SVC(kernel=\"rbf\", probability=True, random_state=RANDOM_SEED))\n",
    "])\n",
    "svm_grid = {\"clf__C\":[5,10], \"clf__gamma\":[\"scale\", 0.001]}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, base, grid in [\n",
    "    (\"RandomForest\", rf_pipe, rf_grid),\n",
    "    (\"SVM_RBF\",      svm_pipe, svm_grid),\n",
    "]:\n",
    "    cms, accs, probas_all, y_true_all, y_pred_all = [], [], [], [], []\n",
    "    for tr, te in skf.split(X, y):\n",
    "        gs = GridSearchCV(base, grid, cv=3, n_jobs=-1, scoring=\"accuracy\", verbose=0)\n",
    "        gs.fit(X[tr], y[tr])\n",
    "        best = gs.best_estimator_\n",
    "        yp = best.predict(X[te])\n",
    "        accs.append(accuracy_score(y[te], yp))\n",
    "        cms.append(confusion_matrix(y[te], yp, labels=classes))\n",
    "        probas_all.append(best.predict_proba(X[te]))\n",
    "        y_true_all.append(y[te]); y_pred_all.append(yp)\n",
    "\n",
    "    mean_cv_acc = float(np.mean(accs))\n",
    "    cm_sum = np.sum(cms, axis=0)\n",
    "\n",
    "    # Confusion matrix saved as PNG (no plt.show())\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_sum, cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix (5-fold CV) — {name}\\nmean acc={mean_cv_acc:.4f}\")\n",
    "    plt.xticks(np.arange(len(classes)), classes, rotation=45)\n",
    "    plt.yticks(np.arange(len(classes)), classes)\n",
    "    for i in range(cm_sum.shape[0]):\n",
    "        for j in range(cm_sum.shape[1]):\n",
    "            plt.text(j, i, str(cm_sum[i, j]), ha='center', va='center', fontsize=8)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    cm_path = os.path.join(DIRS[\"plots\"], f\"cm_{name}.png\")\n",
    "    plt.savefig(cm_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # ROC curve saved as PNG (no plt.show())\n",
    "    proba_stack = np.vstack(probas_all)\n",
    "    y_true_stack = np.concatenate(y_true_all)\n",
    "    Ybin = label_binarize(y_true_stack, classes=classes)\n",
    "    aucs, eers = [], []\n",
    "    plt.figure(figsize=(6,5))\n",
    "    for i, cname in enumerate(classes):\n",
    "        fpr, tpr, thr = roc_curve(Ybin[:, i], proba_stack[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        fnr = 1 - tpr\n",
    "        idx = np.nanargmin(np.abs(fnr - fpr))\n",
    "        eer = float((fnr[idx] + fpr[idx]) / 2.0)\n",
    "        aucs.append(float(roc_auc)); eers.append(float(eer))\n",
    "        plt.plot(fpr, tpr, label=f\"{cname} AUC={roc_auc:.3f}, EER={eer:.3f}\")\n",
    "    plt.plot([0,1],[0,1],'--', lw=1)\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"One-vs-Rest ROC — {name}\")\n",
    "    plt.legend(fontsize=7, loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    roc_png = os.path.join(DIRS[\"plots\"], f\"roc_{name}.png\")\n",
    "    plt.savefig(roc_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Save verification table (per-class + macro row)\n",
    "    verif_csv = os.path.join(DIRS[\"models\"], f\"verification_{name}.csv\")\n",
    "    verif_df = pd.DataFrame({\"Class\": classes, \"AUC\": aucs, \"EER\": eers})\n",
    "    macro_row = pd.DataFrame([{\n",
    "        \"Class\": \"macro_avg\",\n",
    "        \"AUC\": float(np.mean(aucs)),\n",
    "        \"EER\": float(np.mean(eers)),\n",
    "    }])\n",
    "    verif_df = pd.concat([verif_df, macro_row], ignore_index=True)\n",
    "    verif_df.to_csv(verif_csv, index=False)\n",
    "\n",
    "    # Train final best model on ALL data and save\n",
    "    final_gs = GridSearchCV(base, grid, cv=3, n_jobs=-1, scoring=\"accuracy\", verbose=0)\n",
    "    final_gs.fit(X, y)\n",
    "    best_model = final_gs.best_estimator_\n",
    "    model_path = os.path.join(DIRS[\"models\"], f\"best_{name}.joblib\")\n",
    "    joblib.dump(best_model, model_path)\n",
    "\n",
    "    # Classification report (stacked CV preds, indicative)\n",
    "    y_pred_stack = np.concatenate(y_pred_all)\n",
    "    report = classification_report(y_true_stack, y_pred_stack, labels=classes, output_dict=True)\n",
    "    rep_df = pd.DataFrame(report).transpose()\n",
    "    rep_path = os.path.join(DIRS[\"models\"], f\"class_report_{name}.csv\")\n",
    "    rep_df.to_csv(rep_path)\n",
    "\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"mean_cv_acc\": mean_cv_acc,\n",
    "        \"confusion_matrix_png\": cm_path,\n",
    "        \"roc_png\": roc_png,\n",
    "        \"verification_csv\": verif_csv,\n",
    "        \"class_report_csv\": rep_path,\n",
    "        \"saved_model\": model_path,\n",
    "        \"best_params\": final_gs.best_params_,\n",
    "    })\n",
    "    print(f\"[{name}] mean CV acc = {mean_cv_acc:.4f} | best params: {final_gs.best_params_}\")\n",
    "\n",
    "summary = pd.DataFrame(results)\n",
    "summary_path = os.path.join(DIRS[\"models\"], \"model_summary.csv\")\n",
    "summary.to_csv(summary_path, index=False)\n",
    "print(\"\\nSaved model summary:\", summary_path)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5008bde6-a447-4bfb-a9cc-48893a2d9964",
   "metadata": {},
   "source": [
    "### Subject-wise Similarity Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7acd2593-e803-45b3-8181-dcf57d9747c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved similarity matrix & report:\n",
      " - C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\similarity\\subject_similarity_matrix.csv\n",
      " - C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\similarity\\subject_similarity_matrix.png\n",
      " - C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\similarity\\subject_similarity_report.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Genuine_Mean</th>\n",
       "      <th>Genuine_Std</th>\n",
       "      <th>Nearest_Impostor_Subject</th>\n",
       "      <th>Nearest_Impostor_Sim</th>\n",
       "      <th>Safety_Margin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>s21</td>\n",
       "      <td>0.791138</td>\n",
       "      <td>0.067362</td>\n",
       "      <td>s18</td>\n",
       "      <td>0.306389</td>\n",
       "      <td>0.484749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s02</td>\n",
       "      <td>0.909292</td>\n",
       "      <td>0.040191</td>\n",
       "      <td>s28</td>\n",
       "      <td>0.432762</td>\n",
       "      <td>0.476530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>s34</td>\n",
       "      <td>0.745629</td>\n",
       "      <td>0.082809</td>\n",
       "      <td>s00</td>\n",
       "      <td>0.332648</td>\n",
       "      <td>0.412981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>s18</td>\n",
       "      <td>0.811084</td>\n",
       "      <td>0.077162</td>\n",
       "      <td>s11</td>\n",
       "      <td>0.431162</td>\n",
       "      <td>0.379922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>s32</td>\n",
       "      <td>0.843383</td>\n",
       "      <td>0.096294</td>\n",
       "      <td>s12</td>\n",
       "      <td>0.469653</td>\n",
       "      <td>0.373730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>s19</td>\n",
       "      <td>0.836597</td>\n",
       "      <td>0.076883</td>\n",
       "      <td>s16</td>\n",
       "      <td>0.468548</td>\n",
       "      <td>0.368050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>s20</td>\n",
       "      <td>0.783862</td>\n",
       "      <td>0.110625</td>\n",
       "      <td>s04</td>\n",
       "      <td>0.428982</td>\n",
       "      <td>0.354880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s04</td>\n",
       "      <td>0.840077</td>\n",
       "      <td>0.055229</td>\n",
       "      <td>s15</td>\n",
       "      <td>0.523232</td>\n",
       "      <td>0.316845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>s28</td>\n",
       "      <td>0.799017</td>\n",
       "      <td>0.070228</td>\n",
       "      <td>s00</td>\n",
       "      <td>0.511023</td>\n",
       "      <td>0.287994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>s29</td>\n",
       "      <td>0.893851</td>\n",
       "      <td>0.044270</td>\n",
       "      <td>s00</td>\n",
       "      <td>0.613133</td>\n",
       "      <td>0.280718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject  Genuine_Mean  Genuine_Std Nearest_Impostor_Subject  \\\n",
       "21     s21      0.791138     0.067362                      s18   \n",
       "2      s02      0.909292     0.040191                      s28   \n",
       "34     s34      0.745629     0.082809                      s00   \n",
       "18     s18      0.811084     0.077162                      s11   \n",
       "32     s32      0.843383     0.096294                      s12   \n",
       "19     s19      0.836597     0.076883                      s16   \n",
       "20     s20      0.783862     0.110625                      s04   \n",
       "4      s04      0.840077     0.055229                      s15   \n",
       "28     s28      0.799017     0.070228                      s00   \n",
       "29     s29      0.893851     0.044270                      s00   \n",
       "\n",
       "    Nearest_Impostor_Sim  Safety_Margin  \n",
       "21              0.306389       0.484749  \n",
       "2               0.432762       0.476530  \n",
       "34              0.332648       0.412981  \n",
       "18              0.431162       0.379922  \n",
       "32              0.469653       0.373730  \n",
       "19              0.468548       0.368050  \n",
       "20              0.428982       0.354880  \n",
       "4               0.523232       0.316845  \n",
       "28              0.511023       0.287994  \n",
       "29              0.613133       0.280718  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Subject-wise similarity (centroids, margin, top pairs) ---\n",
    "\n",
    "# Requires: X, y, classes, DIRS\n",
    "scaler_sim = StandardScaler(with_mean=True, with_std=True)\n",
    "Xz = scaler_sim.fit_transform(X)\n",
    "\n",
    "subjects = np.array(classes)\n",
    "centroids = {s: Xz[y == s].mean(axis=0) for s in subjects}\n",
    "C = np.vstack([centroids[s] for s in subjects])\n",
    "S_mat = cosine_similarity(C)\n",
    "\n",
    "# Matrix CSV + heatmap\n",
    "sim_df = pd.DataFrame(S_mat, index=subjects, columns=subjects)\n",
    "sim_csv = os.path.join(DIRS[\"similarity\"], \"subject_similarity_matrix.csv\")\n",
    "sim_df.to_csv(sim_csv)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.imshow(S_mat, vmin=-1, vmax=1, cmap=\"coolwarm\")\n",
    "plt.xticks(np.arange(len(subjects)), subjects, rotation=90)\n",
    "plt.yticks(np.arange(len(subjects)), subjects)\n",
    "plt.colorbar(label=\"Cosine similarity\")\n",
    "plt.title(\"Subject–Subject Centroid Similarity\")\n",
    "plt.tight_layout()\n",
    "sim_png = os.path.join(DIRS[\"similarity\"], \"subject_similarity_matrix.png\")\n",
    "plt.savefig(sim_png, dpi=150); plt.close()\n",
    "\n",
    "# Per-subject metrics\n",
    "rows = []\n",
    "for s in subjects:\n",
    "    Xi = Xz[y == s]\n",
    "    sims = cosine_similarity(Xi, centroids[s].reshape(1,-1)).ravel()\n",
    "    genuine_mean = float(np.mean(sims)) if sims.size else np.nan\n",
    "    genuine_std  = float(np.std(sims))  if sims.size else np.nan\n",
    "    row = sim_df.loc[s].drop(labels=s)\n",
    "    nearest_imp_id  = row.idxmax()\n",
    "    nearest_imp_sim = float(row.max())\n",
    "    safety_margin = genuine_mean - nearest_imp_sim if np.isfinite(genuine_mean) else np.nan\n",
    "    rows.append({\n",
    "        \"Subject\": s,\n",
    "        \"Genuine_Mean\": genuine_mean,\n",
    "        \"Genuine_Std\": genuine_std,\n",
    "        \"Nearest_Impostor_Subject\": nearest_imp_id,\n",
    "        \"Nearest_Impostor_Sim\": nearest_imp_sim,\n",
    "        \"Safety_Margin\": safety_margin\n",
    "    })\n",
    "\n",
    "report_df = pd.DataFrame(rows).sort_values(by=\"Safety_Margin\", ascending=False)\n",
    "report_csv = os.path.join(DIRS[\"similarity\"], \"subject_similarity_report.csv\")\n",
    "report_df.to_csv(report_csv, index=False)\n",
    "print(\"Saved similarity matrix & report:\")\n",
    "print(\" -\", sim_csv)\n",
    "print(\" -\", sim_png)\n",
    "print(\" -\", report_csv)\n",
    "report_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52675c2c-7c1f-4ce3-8024-ae18a9cc8f50",
   "metadata": {},
   "source": [
    "## Top Confusing paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfe7827c-5b44-447e-a75d-eb2dd569143c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top confusing pairs: C:\\Users\\Admin\\Desktop\\ALVIN\\outputs\\similarity\\subject_top_confusing_pairs.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_A</th>\n",
       "      <th>Subject_B</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s13</td>\n",
       "      <td>s26</td>\n",
       "      <td>0.776810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s23</td>\n",
       "      <td>s25</td>\n",
       "      <td>0.684611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s06</td>\n",
       "      <td>s11</td>\n",
       "      <td>0.637231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s26</td>\n",
       "      <td>s33</td>\n",
       "      <td>0.632619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s00</td>\n",
       "      <td>s29</td>\n",
       "      <td>0.613133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>s09</td>\n",
       "      <td>s26</td>\n",
       "      <td>0.610011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>s12</td>\n",
       "      <td>s29</td>\n",
       "      <td>0.609457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s09</td>\n",
       "      <td>s13</td>\n",
       "      <td>0.594944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>s08</td>\n",
       "      <td>s33</td>\n",
       "      <td>0.585876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>s09</td>\n",
       "      <td>s33</td>\n",
       "      <td>0.569343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Subject_A Subject_B  Cosine_Similarity\n",
       "0       s13       s26           0.776810\n",
       "1       s23       s25           0.684611\n",
       "2       s06       s11           0.637231\n",
       "3       s26       s33           0.632619\n",
       "4       s00       s29           0.613133\n",
       "5       s09       s26           0.610011\n",
       "6       s12       s29           0.609457\n",
       "7       s09       s13           0.594944\n",
       "8       s08       s33           0.585876\n",
       "9       s09       s33           0.569343"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10\n",
    "pairs = []\n",
    "for i, si in enumerate(subjects):\n",
    "    for j, sj in enumerate(subjects):\n",
    "        if i >= j: \n",
    "            continue\n",
    "        pairs.append((si, sj, float(S_mat[i, j])))\n",
    "\n",
    "pairs_sorted = sorted(pairs, key=lambda t: t[2], reverse=True)\n",
    "top_pairs_df = pd.DataFrame(pairs_sorted[:N], columns=[\"Subject_A\",\"Subject_B\",\"Cosine_Similarity\"])\n",
    "top_pairs_csv = os.path.join(DIRS[\"similarity\"], \"subject_top_confusing_pairs.csv\")\n",
    "top_pairs_df.to_csv(top_pairs_csv, index=False)\n",
    "print(\"Saved top confusing pairs:\", top_pairs_csv)\n",
    "top_pairs_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
